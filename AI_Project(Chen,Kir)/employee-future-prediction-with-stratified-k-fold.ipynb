{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Contributed by Jun Yan Chen & Batuhan Kir\n\n#  Python 3 environment \nimport matplotlib.pyplot as plot\nimport seaborn as sb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndf = pd.read_csv(\"../input/employee-future-prediction/Employee.csv\") \nprint (df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T19:43:07.838263Z","iopub.execute_input":"2021-12-02T19:43:07.838594Z","iopub.status.idle":"2021-12-02T19:43:07.864494Z","shell.execute_reply.started":"2021-12-02T19:43:07.838560Z","shell.execute_reply":"2021-12-02T19:43:07.863523Z"},"trusted":true},"execution_count":450,"outputs":[]},{"cell_type":"markdown","source":"# Check Missing Data","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:07.866348Z","iopub.execute_input":"2021-12-02T19:43:07.866604Z","iopub.status.idle":"2021-12-02T19:43:07.877133Z","shell.execute_reply.started":"2021-12-02T19:43:07.866573Z","shell.execute_reply":"2021-12-02T19:43:07.876423Z"},"trusted":true},"execution_count":451,"outputs":[]},{"cell_type":"markdown","source":"****No missing value in the dataset.","metadata":{}},{"cell_type":"markdown","source":"# Brief summary about each columns in the dataset.\n\n1. Education: Education level of the employee.                  \n2. Joining Year: The year when the employee joined the company.\n3. City: Office location where the information was posted.      \n4. PaymentTier: The employee's level of payment                 \n5. Age: The age of the employee.                                \n6. Gender: Gender of the employee.                              \n7. EverBenched: Indicates whether the employee experienced being benched. (Not active on any project)             \n8. ExperienceInCurrentDomain: Number of years of the employee's experience in current field. \n9. LeaveOrNot: Whether the employee left the company within 2 years. ","metadata":{}},{"cell_type":"markdown","source":"# Attributes vs. LeaveOrNot","metadata":{}},{"cell_type":"code","source":"education_vs_leave = df.groupby(['Education', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'Education', y = 'Count', hue = 'LeaveOrNot', data = education_vs_leave)\nplot.title(\"Education VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:07.878583Z","iopub.execute_input":"2021-12-02T19:43:07.879019Z","iopub.status.idle":"2021-12-02T19:43:08.154450Z","shell.execute_reply.started":"2021-12-02T19:43:07.878986Z","shell.execute_reply":"2021-12-02T19:43:08.153603Z"},"trusted":true},"execution_count":452,"outputs":[]},{"cell_type":"markdown","source":"There are more Bachelors in the dataset over all and larger portion of all three categories did not leave in 2 years, the amount of samples with Masters education that chose to leave is higher than Bachelors and PhD.","metadata":{}},{"cell_type":"code","source":"joining_vs_leave = df.groupby(['JoiningYear', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'JoiningYear', y = 'Count', hue = 'LeaveOrNot', data = joining_vs_leave)\nplot.title(\"Joining Year VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:08.155836Z","iopub.execute_input":"2021-12-02T19:43:08.157901Z","iopub.status.idle":"2021-12-02T19:43:08.496721Z","shell.execute_reply.started":"2021-12-02T19:43:08.157863Z","shell.execute_reply":"2021-12-02T19:43:08.495850Z"},"trusted":true},"execution_count":453,"outputs":[]},{"cell_type":"markdown","source":"During year 2012-2017, majority did not leave within 2 years, the result is reverse in 2018.","metadata":{}},{"cell_type":"code","source":"city_vs_leave = df.groupby(['City', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'City', y = 'Count', hue = 'LeaveOrNot', data = city_vs_leave)\nplot.title(\"City VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:08.499162Z","iopub.execute_input":"2021-12-02T19:43:08.499496Z","iopub.status.idle":"2021-12-02T19:43:08.774951Z","shell.execute_reply.started":"2021-12-02T19:43:08.499460Z","shell.execute_reply":"2021-12-02T19:43:08.774028Z"},"trusted":true},"execution_count":454,"outputs":[]},{"cell_type":"markdown","source":"Sample with city Pune who left within 2 years is slightly higher than who stayed.\nMore samples in Bangalore and New Delhi chose to stay.","metadata":{}},{"cell_type":"code","source":"tier_vs_leave = df.groupby(['PaymentTier', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'PaymentTier', y = 'Count', hue = 'LeaveOrNot', data = tier_vs_leave)\nplot.title(\"Payment Tier VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:08.776428Z","iopub.execute_input":"2021-12-02T19:43:08.776766Z","iopub.status.idle":"2021-12-02T19:43:09.039463Z","shell.execute_reply.started":"2021-12-02T19:43:08.776727Z","shell.execute_reply":"2021-12-02T19:43:09.038877Z"},"trusted":true},"execution_count":455,"outputs":[]},{"cell_type":"markdown","source":"Samples with PaymentTier 3 has the highest distribution. The amount of payment is ranked as 1>2>3, payment tier with lower value means higher pay.","metadata":{}},{"cell_type":"code","source":"age_vs_leave = df.groupby(['Age', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'Age', y = 'Count', hue = 'LeaveOrNot', data = age_vs_leave)\nplot.title(\"Age VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:09.040683Z","iopub.execute_input":"2021-12-02T19:43:09.041094Z","iopub.status.idle":"2021-12-02T19:43:09.548106Z","shell.execute_reply.started":"2021-12-02T19:43:09.041043Z","shell.execute_reply":"2021-12-02T19:43:09.546831Z"},"trusted":true},"execution_count":456,"outputs":[]},{"cell_type":"markdown","source":"Age ranged from 22 to 41. Large amount of samples are in the range 24-30. Age 22 and 23 have the smallest amount of samples overall.","metadata":{}},{"cell_type":"code","source":"gender_vs_leave = df.groupby(['Gender', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'Gender', y = 'Count', hue = 'LeaveOrNot', data = gender_vs_leave)\nplot.title(\"Gender VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:09.549983Z","iopub.execute_input":"2021-12-02T19:43:09.550809Z","iopub.status.idle":"2021-12-02T19:43:09.815448Z","shell.execute_reply.started":"2021-12-02T19:43:09.550755Z","shell.execute_reply":"2021-12-02T19:43:09.814560Z"},"trusted":true},"execution_count":457,"outputs":[]},{"cell_type":"markdown","source":"There are more males than females in the dataset, males who stayed have a larger distribution than males who left. Females have a fairly close distribution between staying and leaving.","metadata":{}},{"cell_type":"code","source":"benched_vs_leave = df.groupby(['EverBenched', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'EverBenched', y = 'Count', hue = 'LeaveOrNot', data = benched_vs_leave)\nplot.title(\"Benched VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:09.817142Z","iopub.execute_input":"2021-12-02T19:43:09.817708Z","iopub.status.idle":"2021-12-02T19:43:10.069344Z","shell.execute_reply.started":"2021-12-02T19:43:09.817669Z","shell.execute_reply":"2021-12-02T19:43:10.068411Z"},"trusted":true},"execution_count":458,"outputs":[]},{"cell_type":"markdown","source":"The majority of the people in the data had never been benched, most stayed. People who was benched before have closer count between stay and leave.","metadata":{}},{"cell_type":"code","source":"experience_vs_leave = df.groupby(['ExperienceInCurrentDomain', 'LeaveOrNot']).size().reset_index(name = 'Count')\nplot.figure(figsize = (20,10))\nsb.barplot(x = 'ExperienceInCurrentDomain', y = 'Count', hue = 'LeaveOrNot', data = experience_vs_leave)\nplot.title(\"Experience VS. Leave or Not\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:10.070659Z","iopub.execute_input":"2021-12-02T19:43:10.070988Z","iopub.status.idle":"2021-12-02T19:43:10.417688Z","shell.execute_reply.started":"2021-12-02T19:43:10.070953Z","shell.execute_reply":"2021-12-02T19:43:10.416645Z"},"trusted":true},"execution_count":459,"outputs":[]},{"cell_type":"markdown","source":"More people chose to stay with any years of experience, the data has less samples on 6 and 7 years of experience.","metadata":{}},{"cell_type":"code","source":"plot.figure(figsize = (10,10))\nplot.title('Distribution whether a employee will leave or not')\ntempdf = df['LeaveOrNot']\ndf['LeaveOrNot'] = df['LeaveOrNot'].map({1:'Leave' ,0:'NotLeave' })\nsb.countplot(x = df['LeaveOrNot'])\nplot.show()\ndf['LeaveOrNot'] = df['LeaveOrNot'].map({'Leave':1 ,'NotLeave':0 })  #reverting back","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:10.419153Z","iopub.execute_input":"2021-12-02T19:43:10.419434Z","iopub.status.idle":"2021-12-02T19:43:10.620289Z","shell.execute_reply.started":"2021-12-02T19:43:10.419401Z","shell.execute_reply":"2021-12-02T19:43:10.619424Z"},"trusted":true},"execution_count":460,"outputs":[]},{"cell_type":"markdown","source":"The count is higher in the data for people who chose to not leave than those who chose to leave.","metadata":{}},{"cell_type":"markdown","source":"# Data Normalization","metadata":{}},{"cell_type":"code","source":"# change categorical data to numerical values\ndata = pd.DataFrame(df)\ndata['Education'] = data['Education'].astype('category')\ndata['Education'] = data['Education'].cat.codes\ndata['City'] = data['City'].astype('category')\ndata['City'] = data['City'].cat.codes\ndata['Gender'] = data['Gender'].astype('category')\ndata['Gender'] = data['Gender'].cat.codes\ndata['EverBenched'] = data['EverBenched'].astype('category')\ndata['EverBenched'] = data['EverBenched'].cat.codes\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:10.621514Z","iopub.execute_input":"2021-12-02T19:43:10.622213Z","iopub.status.idle":"2021-12-02T19:43:10.646435Z","shell.execute_reply.started":"2021-12-02T19:43:10.622180Z","shell.execute_reply":"2021-12-02T19:43:10.645395Z"},"trusted":true},"execution_count":461,"outputs":[]},{"cell_type":"markdown","source":"Dataset after all categorical String data are converted to numerical values.\n* Education: 0:Bachelors, 1:Masters, 2:PhD\n* City: 0：Bangalore, 1:New Delhi, 2:Pune\n* Gender: 0:Female, 1:Male\n* EverBenched: 0:No, 1:Yes","metadata":{}},{"cell_type":"code","source":"sb.scatterplot(x=data['Education'], y=data['City'], hue=data['LeaveOrNot'])","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:10.647741Z","iopub.execute_input":"2021-12-02T19:43:10.648083Z","iopub.status.idle":"2021-12-02T19:43:11.419912Z","shell.execute_reply.started":"2021-12-02T19:43:10.648049Z","shell.execute_reply":"2021-12-02T19:43:11.419268Z"},"trusted":true},"execution_count":462,"outputs":[]},{"cell_type":"markdown","source":"Scatterplots are not good visual representation for most of the features in this datasets becasue most features are categorical, meaning limited range and variations.","metadata":{}},{"cell_type":"markdown","source":"# Classification Models","metadata":{}},{"cell_type":"code","source":"#classification imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:11.422315Z","iopub.execute_input":"2021-12-02T19:43:11.422971Z","iopub.status.idle":"2021-12-02T19:43:11.429469Z","shell.execute_reply.started":"2021-12-02T19:43:11.422921Z","shell.execute_reply":"2021-12-02T19:43:11.428535Z"},"trusted":true},"execution_count":463,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix:\n\n> True Positive: The classifier predicted NotLeave and the passenger actually NotLeave.\n> \n> True Negative: The classifier predicted Leave and the passenger actually Leave.\n> \n> False Postiive: The classifier predicted NotLeave but the passenger actually Leave.\n> \n> False Negative: The classifier predicted Leave but the passenger actually NotLeave.","metadata":{}},{"cell_type":"markdown","source":"# Stratified K-Fold split","metadata":{}},{"cell_type":"markdown","source":"Stratified K-Fold split separate the dataset into 5 different folds, each fold will contain 1/5 of the total data. The data distributions in each fold will remain close to another fold. For example, it will not exist a situation where fold 1 contains 100 Masters while fold 2 only contains 3 Masters. The amount of data are not splited to have exactly equal amount in each fold but they will have close counts.\n","metadata":{}},{"cell_type":"code","source":"X = data.drop(columns = 'LeaveOrNot')\ny = data['LeaveOrNot']\n# Stratified K-Fold Cross Validation  k=5\ncv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:11.431204Z","iopub.execute_input":"2021-12-02T19:43:11.431447Z","iopub.status.idle":"2021-12-02T19:43:11.442753Z","shell.execute_reply.started":"2021-12-02T19:43:11.431419Z","shell.execute_reply":"2021-12-02T19:43:11.441933Z"},"trusted":true},"execution_count":464,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# Random Forest with Stratified K-Fold CV\nrdForest = RandomForestClassifier()\nscore = cross_val_score(rdForest, X, y, scoring='accuracy', cv=cv, n_jobs=-1) #n_jobs=number of jobs running parallel\nprint(score)\nprint('Accuracy: %.3f (%.3f)' % (score.mean(), score.std()))\ny_pred = cross_val_predict(rdForest, X, y, cv=cv)\n# Random Forest Confusion Matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint('Confusion Matrix: ')\nrfmean = round(score.mean(),3)\nrfstd = round(score.std(),3)\nplot.figure(figsize=(9,9))\nsb.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplot.ylabel('Actual label');\nplot.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(rfmean)\nplot.title(all_sample_title, size = 15);","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:11.444264Z","iopub.execute_input":"2021-12-02T19:43:11.444850Z","iopub.status.idle":"2021-12-02T19:43:14.718603Z","shell.execute_reply.started":"2021-12-02T19:43:11.444809Z","shell.execute_reply":"2021-12-02T19:43:14.717099Z"},"trusted":true},"execution_count":465,"outputs":[]},{"cell_type":"markdown","source":"# Stratified K-Fold CV Feature Importances","metadata":{}},{"cell_type":"markdown","source":"Feature Importances: Each feature rated in the range of 0 to 1 based on how much the model depends on that feature. The feature with the highest score shows that it is the most important feature for the model to consider when making a prediction. All values add up to 1.","metadata":{}},{"cell_type":"code","source":"# Stratified K-Fold CV Feature Importances with Random Forest\nfrom sklearn.datasets import make_classification\nfeature_names = list(X.columns)\n# classification dataset\ndata_x, data_y = make_classification(n_features=9)\ncount = 1\nfor train, _ in cv.split(data_x, data_y):\n    rdForest.fit(data_x[train, :], data_y[train])\n    importances_index_desc = np.argsort(rdForest.feature_importances_)[::-1]\n    feature_labels = [feature_names[-i] for i in importances_index_desc]\n\n    # plot\n    plot.figure()\n    plot.bar(feature_labels, rdForest.feature_importances_[importances_index_desc])\n    plot.xticks(feature_labels, rotation='vertical')\n    plot.ylabel('Importance')\n    plot.xlabel('Features')\n    plot.title('Fold {}'.format(count))\n    count = count + 1\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:14.719995Z","iopub.execute_input":"2021-12-02T19:43:14.721059Z","iopub.status.idle":"2021-12-02T19:43:17.066639Z","shell.execute_reply.started":"2021-12-02T19:43:14.721008Z","shell.execute_reply":"2021-12-02T19:43:17.065734Z"},"trusted":true},"execution_count":466,"outputs":[]},{"cell_type":"markdown","source":"# Regular K-Fold CV Feature Importances","metadata":{}},{"cell_type":"markdown","source":"Regular K-Fold is also tested to show the differences. Both Regular and Stratified K-fold methods use the Random Forest model. Unlike the Stratified method, the regular K-fold method split the data totally random, meaning it would not take consideration on how balanced the dataset is. Each fold will have a random distribution of data. It is possible for regular K-fold split to have all the samples with PhD in fold 1 and no sample with PhD in fold 2 at all. This can cause the problem of overfitting, but using random forest on this method is able to improve this to a degree.","metadata":{}},{"cell_type":"code","source":"# Regular K-Fold CV Feature Importances with Random Forest\ncvn = KFold(n_splits=5, random_state=42, shuffle=True)\nrdForest = RandomForestClassifier()\nscore = cross_val_score(rdForest, X, y, scoring='accuracy', cv=cvn, n_jobs=-1)\nprint(score)\nprint('Accuracy: %.3f (%.3f)' % (score.mean(), score.std()))\ny_pred = cross_val_predict(rdForest, X, y, cv=cvn)\n\n# Confusion Matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint('Confusion Matrix: ')\nplot.figure(figsize=(9,9))\nsb.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplot.ylabel('Actual label');\nplot.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(score.mean())\nplot.title(all_sample_title, size = 15);\n\nfeature_names = list(X.columns)\n# classification dataset\ndata_x, data_y = make_classification(n_features=9)\ncount = 1\nfor train, _ in cvn.split(data_x, data_y):\n    rdForest.fit(data_x[train, :], data_y[train])\n    importances_index_desc = np.argsort(rdForest.feature_importances_)[::-1]\n    feature_labels = [feature_names[-i] for i in importances_index_desc]\n\n    # plot\n    plot.figure()\n    plot.bar(feature_labels, rdForest.feature_importances_[importances_index_desc])\n    plot.xticks(feature_labels, rotation='vertical')\n    plot.ylabel('Importance')\n    plot.xlabel('Features')\n    plot.title('Fold {}'.format(count))\n    count = count + 1\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:17.068266Z","iopub.execute_input":"2021-12-02T19:43:17.068526Z","iopub.status.idle":"2021-12-02T19:43:22.530277Z","shell.execute_reply.started":"2021-12-02T19:43:17.068494Z","shell.execute_reply":"2021-12-02T19:43:22.529399Z"},"trusted":true},"execution_count":467,"outputs":[]},{"cell_type":"markdown","source":"As a result, the most important feature for Stratified K-fold split with most likely be the same for every fold because the data was split with a pre condition that all folds will have similar distributions. While for regular K-fold, the most important feature can sometimes varies between folds.","metadata":{}},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"# DTree with Stratified K-Fold CV\ndTree = DecisionTreeClassifier()\nscore = cross_val_score(dTree, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(score)\nprint('Accuracy: %.3f (%.3f)' % (score.mean(), score.std()))\ny_pred = cross_val_predict(dTree, X, y, cv=cv)\n#Confusion Matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint('Confusion Matrix: ')\ndtmean = round(score.mean(),3)\ndtstd = round(score.std(),3)\nplot.figure(figsize=(9,9))\nsb.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplot.ylabel('Actual label');\nplot.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(dtmean)\nplot.title(all_sample_title, size = 15);","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:22.531810Z","iopub.execute_input":"2021-12-02T19:43:22.532323Z","iopub.status.idle":"2021-12-02T19:43:22.921702Z","shell.execute_reply.started":"2021-12-02T19:43:22.532286Z","shell.execute_reply":"2021-12-02T19:43:22.920801Z"},"trusted":true},"execution_count":468,"outputs":[]},{"cell_type":"markdown","source":"# Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Gaussian Naive Bayes with Stratified K-Fold CV\nnaiveB = GaussianNB()\nscore = cross_val_score(naiveB, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \nprint(score)\nprint('Accuracy: %.3f (%.3f)' % (score.mean(), score.std()))\ny_pred = cross_val_predict(naiveB, X, y, cv=cv)\n#Confusion Matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint('Confusion Matrix: ')\nnbmean = round(score.mean(),3)\nnbstd = round(score.std(),3)\nplot.figure(figsize=(9,9))\nsb.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplot.ylabel('Actual label');\nplot.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(nbmean)\nplot.title(all_sample_title, size = 15);","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:22.922956Z","iopub.execute_input":"2021-12-02T19:43:22.923226Z","iopub.status.idle":"2021-12-02T19:43:23.295301Z","shell.execute_reply.started":"2021-12-02T19:43:22.923193Z","shell.execute_reply":"2021-12-02T19:43:23.294192Z"},"trusted":true},"execution_count":469,"outputs":[]},{"cell_type":"markdown","source":"// Didn't use linear regression because we are dealing wth categorical data.","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regresssion with Stratified K-Fold CV\nlogR = LogisticRegression(solver='liblinear')\nscore = cross_val_score(logR, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \nprint(score)\nprint('Accuracy: %.3f (%.3f)' % (score.mean(), score.std()))\ny_pred = cross_val_predict(logR, X, y, cv=cv)\n#Confusion Matrix\nconf_mat = confusion_matrix(y, y_pred)\nprint('Confusion Matrix: ')\nlrmean = round(score.mean(),3)\nlrstd = round(score.std(),3)\nplot.figure(figsize=(9,9))\nsb.heatmap(conf_mat, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplot.ylabel('Actual label');\nplot.xlabel('Predicted label');\nall_sample_title = 'Accuracy Score: {0}'.format(lrmean)\nplot.title(all_sample_title, size = 15);","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:23.296638Z","iopub.execute_input":"2021-12-02T19:43:23.296989Z","iopub.status.idle":"2021-12-02T19:43:23.728026Z","shell.execute_reply.started":"2021-12-02T19:43:23.296954Z","shell.execute_reply":"2021-12-02T19:43:23.727090Z"},"trusted":true},"execution_count":470,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"# Table labeling all models used \nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gaussian Naive Bayes'],\n    \n    'Score': [lrmean, dtmean, rfmean,  nbmean],\n    'SD': [lrstd, dtstd, rfstd,  nbstd]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T19:43:23.729921Z","iopub.execute_input":"2021-12-02T19:43:23.730281Z","iopub.status.idle":"2021-12-02T19:43:23.745715Z","shell.execute_reply.started":"2021-12-02T19:43:23.730230Z","shell.execute_reply":"2021-12-02T19:43:23.744836Z"},"trusted":true},"execution_count":471,"outputs":[]},{"cell_type":"markdown","source":"Models are ranked above based on their accuracy scores","metadata":{}},{"cell_type":"markdown","source":"Random Forest has the highest accuracy score followed by Decision Tree.","metadata":{}},{"cell_type":"markdown","source":"1. Logistic regression is expected to have better performance when the dataset is balanced and mostly only contains binary dependent variable(variable that presents two outcome, 0 and 1).\n2. Gaussian Naive Bayes assumes the dataset is normally distributed when predicting and it would have better performance if all features in the data are continues, this contrast from our dataset which is strongly unbalanced and where most features are discrete and categorical.\n3. Decision Tree has a good performance on the dataset due to a small amount of columns, which means it has a fairly small depth. When a decision tree has more depth, it would increase the complexity for the model, a higher complexity can easily result in overfitting when using decision tree classifier.\n4. Random forest is a combination of many decision tree. The prediction results are based on the maximum vote among the trained trees. It further reduces the chance of overfitting. When combining with Stratified K-Fold Cross Validation, it does not necessarily increase the accuracy of the model, but the precision will be greatly increased.\n","metadata":{}}]}