# -*- coding: utf-8 -*-
"""ML model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C13VxgHY6rIZ9E5yDioyDtGJh227BqKK

# Demo: 
## Machine learning models
"""

# Commented out IPython magic to ensure Python compatibility.
from numpy.core.fromnumeric import mean, std
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import model_selection
# the following line allows ipython to display plots
# %matplotlib inline

# Reading file from google drive



"""`cars.csv` is in an easy-to-read comma separated format and the following *pandas* functionality makes it easy to read it into a `DataFrame` object. """

# read this csv file, remember to put the full path to 
# the directory where you saved the data
df = pd.read_csv('D:\Rowan\Concepts in AI\AI_ML\cars.csv')  # df is DataFrame object
print (df.head())    # see the first 5 rows of the loaded table

print ("Data type: ") 
print (df.dtypes )

"""Scatterplot between MPG and Weight attributes:"""

plt.figure(figsize=(5,3))
plt.scatter(df['MPG'],df['Weight'], color='blue', alpha=0.2);
plt.xlabel("MPG")
plt.ylabel("weight")



from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LinearRegression

'''
df.loc[df['Origin'] == 'Japan', 'label'] = 1  
df.loc[df['Origin'] == 'Europe', 'label'] = 2 
df.loc[df['Origin'] == 'US', 'label'] = 3 

print ( df['label'].value_counts(ascending=True) )
print ("Baseline accuracy: ")
print (254/(73+79+254))  # 0.62
# D tree is fine with this
'''

df.loc[df['Origin'] != 'US', 'label'] = int(0) 
df.loc[df['Origin'] == 'US', 'label'] = int(1)


data = df[['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model', 'label' ]]
X_train, X_test = train_test_split( data ,test_size=0.1, random_state=42)

train_bags   = X_train[ [ 'MPG', 'Cylinders', 'Displacement','Horsepower', 'Weight'] ]
train_labels =  X_train[ ['label'] ]

test_bags   = X_test[ [ 'MPG', 'Cylinders', 'Displacement','Horsepower', 'Weight'] ]
test_labels = X_test[ ['label'] ]

print (train_bags[:10])  # check first three data
print (train_labels[:10])  # check first three data

print ("Showing data labels: ")
print ( df['label'].value_counts(ascending=True) )

print ("Baseline accuracy: ")
print (254/(254+152))

print ("Model: Linear regression")
reg = LinearRegression()
reg.fit(train_bags, train_labels)
predictions = reg.predict( test_bags )
prediction_label = [1 if x>0.7 else 0 for x in predictions ]
print ("Accuracy:" + str ( round( accuracy_score(test_labels, prediction_label) , 4) ) )

from sklearn import tree
print ("*" * 25)
print ("Model: Decision Tree")
dTree = DecisionTreeClassifier( )         #max_depth= 5
dTree.fit(train_bags, train_labels)
text_representation = tree.export_text(dTree)
print(text_representation)
predictions = dTree.predict(test_bags)
print ("Accuracy:" + str ( round( accuracy_score(test_labels, predictions) , 4) ) )


from sklearn.ensemble import RandomForestClassifier
print ("*" * 25)
print ("Model: Random Forest")
model = RandomForestClassifier()
model.fit(train_bags, train_labels.values.ravel())
pred = model.predict(test_bags)
print ("Accuracy:" + str ( round( accuracy_score(test_labels, pred) , 4) ) )

"""
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict

X = np.array(df[['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight']])
y = np.array(df['label'])
X_train, X_test, y_train, y_test = train_test_split( X, y ,test_size=0.1, random_state=42)
print ("*" * 25)
print ("Model: Random Forest k fold")
model = RandomForestClassifier()
model.fit(X_train, y_train)
score_test = cross_val_score(model, X_test, y_test, scoring='accuracy' , cv = 5)
print (np.mean(score_test))
"""



"""**Questions:**

Name: Jun Yan Chen
Date: 11/1/2021

1) Run the models including other attributes and report the result.
    The model accuracy greatly increased frp, 0.8049 to 0.9756 when we include the attribute 'Displacement'.
    But the accuracy does not change when either attribute 'Acceleration' or 'Model' is added,
    the accuracy only increased to 0.8293 when both 'Acceleration' and 'Model' are included.
    The combination of 'Displacement' and 'Acceleration' brings the accuracy up to 0.9024, and
    'Displacement' and 'Model' gives a result of 0.9268.
    Linear regression remains unchanged.
    Attribute 'Origin' or 'label' brings both DTree and Linear regression to 1.0.

2) Do you see any improvements? Explain the reasons behind the results.
    From the results, only the attribute 'Displacement' provides a significant improve to the model accuracy.
    It is to believed that the attribute 'Displacement' have the most impact on the model accuracy because
    the displacement standards and taxes, etc. are very different for most countries, which making it a very important
    factor for the training model.
    While the attributes 'Acceleration' and 'Model' have little to almost no significant variations between the countries,
    they did not provide strong enough information for the model, and was not able to increase the model accuracy.
    And of course 'label' is the result we are finding, adding it will automatically brings the accuracy to 100%

3) Do you have any other suggestions to improve the model accuracy?
    This dataset only have 406 total items which is relatively small, and the decision tree model we have is generally 
    better when the dataset gets larger, decision tree also has a higher chance of overfitting. 
    Because of this we can try using the random forest classification to try improving the model accuracy.
    Another thing we can do is to increase the data size but we cannot do that for the purpose of this assignment.

4) Run any other classifier method (i.e. random forest) and report the result with explanation. 
    By implementing the random forest classification, and comparing the result to Decision Tree for 20 times we get:
    (0 time) Decision Tree Accuracy = 1.0
    (3 time) Decision Tree Accuracy > Random Forest Accuracy
    (8 times) Decision Tree Accuracy = Random Forest Accuracy
    (7 times) Decision Tree Accuracy < Random Forest Accuracy
    (2 time) Random Forest Accuracy = 1.0
    From this result it seems that the Random Forest does provide a better Accuracy than Decision Tree in general.
    Because we have a small dataset, and the data split is random every time, we cannot guarantee the same outcome for every run,
    and both methods have the same accuracy most of the time.
    But in general, When training data is good for Random Forest it will get a better accuracy than Decision Tree 
    or even reach 1.0, which never occured for Decision Tree.
    We can believe that the difference between the two methods will be more obvious if we have a larger dataset.

5) Do you think neural network would be a good model for this problem? Justify your answer.
    I don't think neural network would be a model that we want to use for this problem.
    Because in general we want to use neural network when we have a larger and more complex dataset.
    For the cars.csv, a neural network can provide a fine accuracy, but it would be better if we have a massive dataset.
    For a dataset that contains only 406 items and 7-8 attributes, it might not be efficient to use a neural network over a 
    decision tree or random forest because the data we have is very simple, and it is usually more costly to train a neural network.
"""